{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.transforms import functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def save_data(images, labels, dir, name, split, center):\n",
    "\n",
    "    save_as = f\"{name.lower()}_{split}_{center}.npz\"\n",
    "    np.savez(os.path.join(dir, save_as), \n",
    "                images=images,\n",
    "                labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data/CIFAR10\"\n",
    "image_size = 32\n",
    "number_of_labels = 10\n",
    "patch_size = 4\n",
    "labels = [i for i in range(number_of_labels)]\n",
    "\n",
    "train_files = [f\"data/CIFAR10/cifar-10-batches-py/data_batch_{i}\" for i in range(1,6)]\n",
    "test_file = \"data/CIFAR10/cifar-10-batches-py/test_batch\"\n",
    "\n",
    "train_images = np.concatenate([unpickle(file)[b\"data\"] for file in train_files]).reshape(-1,3,32,32)\n",
    "train_labels = np.concatenate([unpickle(file)[b\"labels\"] for file in train_files])\n",
    "\n",
    "test_images = np.array(unpickle(test_file)[b\"data\"].reshape(-1,3,32,32))\n",
    "test_labels = np.array(unpickle(test_file)[b\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_subset(images, labels, centers_per_group):\n",
    "\n",
    "    images, labels = shuffle(images, labels)\n",
    "\n",
    "    split_labels = np.array_split(labels, centers_per_group)\n",
    "    split_images = np.array_split(images, centers_per_group)\n",
    "\n",
    "    return split_images, split_labels\n",
    "\n",
    "def shuffle(images, labels):\n",
    "    n_samples = len(labels)\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "\n",
    "    labels = labels[shuffled_indices]\n",
    "    images = images[shuffled_indices]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def rotate(dataset, rotation_angle, border_size=0):\n",
    "    images = []\n",
    "    for img in dataset:\n",
    "        img = img.transpose(1,2,0)\n",
    "        img = Image.fromarray(img, mode='RGB')\n",
    "        img = ImageOps.expand(img, border=border_size, fill='black')\n",
    "\n",
    "        # Apply rotation transformation using torchvision's functional API\n",
    "        rotated_img = TF.rotate(img, rotation_angle, expand=True)\n",
    "        # Convert to numpy array and store\n",
    "        images.append(np.array(rotated_img))  # Convert tensor to numpy array\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center: 0 train 4500 val 500\n",
      "center: 1 train 4500 val 500\n",
      "center: 2 train 4500 val 500\n",
      "center: 3 train 4500 val 500\n",
      "center: 4 train 4500 val 500\n",
      "center: 5 train 4500 val 500\n",
      "center: 6 train 4500 val 500\n",
      "center: 7 train 4500 val 500\n",
      "center: 8 train 4500 val 500\n",
      "center: 9 train 4500 val 500\n",
      "center: 0 test 1000\n",
      "center: 1 test 1000\n",
      "center: 2 test 1000\n",
      "center: 3 test 1000\n",
      "center: 4 test 1000\n",
      "center: 5 test 1000\n",
      "center: 6 test 1000\n",
      "center: 7 test 1000\n",
      "center: 8 test 1000\n",
      "center: 9 test 1000\n",
      "center: 10 train 4500 val 500\n",
      "center: 11 train 4500 val 500\n",
      "center: 12 train 4500 val 500\n",
      "center: 13 train 4500 val 500\n",
      "center: 14 train 4500 val 500\n",
      "center: 15 train 4500 val 500\n",
      "center: 16 train 4500 val 500\n",
      "center: 17 train 4500 val 500\n",
      "center: 18 train 4500 val 500\n",
      "center: 19 train 4500 val 500\n",
      "center: 10 test 1000\n",
      "center: 11 test 1000\n",
      "center: 12 test 1000\n",
      "center: 13 test 1000\n",
      "center: 14 test 1000\n",
      "center: 15 test 1000\n",
      "center: 16 test 1000\n",
      "center: 17 test 1000\n",
      "center: 18 test 1000\n",
      "center: 19 test 1000\n",
      "center: 20 train 4500 val 500\n",
      "center: 21 train 4500 val 500\n",
      "center: 22 train 4500 val 500\n",
      "center: 23 train 4500 val 500\n",
      "center: 24 train 4500 val 500\n",
      "center: 25 train 4500 val 500\n",
      "center: 26 train 4500 val 500\n",
      "center: 27 train 4500 val 500\n",
      "center: 28 train 4500 val 500\n",
      "center: 29 train 4500 val 500\n",
      "center: 20 test 1000\n",
      "center: 21 test 1000\n",
      "center: 22 test 1000\n",
      "center: 23 test 1000\n",
      "center: 24 test 1000\n",
      "center: 25 test 1000\n",
      "center: 26 test 1000\n",
      "center: 27 test 1000\n",
      "center: 28 test 1000\n",
      "center: 29 test 1000\n",
      "center: 30 train 4500 val 500\n",
      "center: 31 train 4500 val 500\n",
      "center: 32 train 4500 val 500\n",
      "center: 33 train 4500 val 500\n",
      "center: 34 train 4500 val 500\n",
      "center: 35 train 4500 val 500\n",
      "center: 36 train 4500 val 500\n",
      "center: 37 train 4500 val 500\n",
      "center: 38 train 4500 val 500\n",
      "center: 39 train 4500 val 500\n",
      "center: 30 test 1000\n",
      "center: 31 test 1000\n",
      "center: 32 test 1000\n",
      "center: 33 test 1000\n",
      "center: 34 test 1000\n",
      "center: 35 test 1000\n",
      "center: 36 test 1000\n",
      "center: 37 test 1000\n",
      "center: 38 test 1000\n",
      "center: 39 test 1000\n"
     ]
    }
   ],
   "source": [
    "# all_cases = ['max', 'no', 'min', 'two']\n",
    "all_cases = ['max']\n",
    "all_centers_per_group = [10]\n",
    "\n",
    "for case in all_cases:\n",
    "    for centers_per_group in all_centers_per_group:\n",
    "\n",
    "        if case == 'max':\n",
    "            # rot_groups = [45, 135, 225, 315] # max\n",
    "            rot_groups = [0,90,180,270] # max\n",
    "        elif case == 'no':\n",
    "            rot_groups = [0, 0, 0, 0] # no\n",
    "        elif case == 'min':\n",
    "            # rot_groups = [42, 44, 46, 48] # min\n",
    "            rot_groups = [-3, -1, 1, 3] # min\n",
    "        elif case == 'two':\n",
    "            rot_groups = [-3, 3, 177, 183] # two\n",
    "        else:\n",
    "            exit()\n",
    "\n",
    "        border_size = 0\n",
    "        if case == 'min':\n",
    "            border_size = 2\n",
    "\n",
    "        dest_dir = f'data/CIFAR10/rotcifar10hard{case}{centers_per_group}c'\n",
    "        if not os.path.exists(dest_dir):\n",
    "            os.makedirs(dest_dir)\n",
    "        else:\n",
    "            print(\"Warning, existing files might be overwritten\")\n",
    "\n",
    "        # duplicate and shuffle data so that we have num_cluster x data_len data\n",
    "        shuffled_trains = [(shuffle(train_images, train_labels)) for _ in range(len(rot_groups))]\n",
    "        shuffled_tests = [(shuffle(test_images, test_labels)) for _ in range(len(rot_groups))]\n",
    "\n",
    "        # go through each rotation\n",
    "        for i, rot_group in enumerate(rot_groups):\n",
    "\n",
    "            grouped_train_images, grouped_train_labels = shuffled_trains[i]\n",
    "            # split data into centers_per_group chunks\n",
    "            split_train_images, split_train_labels = get_shuffled_subset(grouped_train_images, grouped_train_labels, centers_per_group)\n",
    "            for j, (imgs, lbls) in enumerate(zip(split_train_images, split_train_labels)):\n",
    "                imgs = rotate(imgs, rot_group, border_size=border_size)\n",
    "                X_train, X_val, y_train, y_val = train_test_split(imgs, lbls, test_size=0.1, random_state=SEED)\n",
    "                center_num = i * centers_per_group + j\n",
    "                save_data(X_train, y_train, dest_dir, \"rotcifar10\", \"train\", center_num)\n",
    "                save_data(X_val, y_val, dest_dir, \"rotcifar10\", \"val\", center_num)\n",
    "                print(\"center:\", center_num, \"train\", len(y_train), \"val\", len(y_val))\n",
    "\n",
    "            # same for test\n",
    "            grouped_test_images, grouped_test_labels = shuffled_tests[i]\n",
    "            split_test_images, split_test_labels = get_shuffled_subset(grouped_test_images, grouped_test_labels, centers_per_group)\n",
    "            for j, (imgs, lbls) in enumerate(zip(split_test_images, split_test_labels)):\n",
    "                imgs = rotate(imgs, rot_group, border_size=border_size)\n",
    "                center_num = i * centers_per_group + j\n",
    "                save_data(imgs, lbls, dest_dir, \"rotcifar10\", \"test\", center_num)\n",
    "                print(\"center:\", center_num, \"test\", len(lbls))\n",
    "\n",
    "        duplicated_rot_groups = [group for group in rot_groups for _ in range(centers_per_group)]\n",
    "\n",
    "        log = [[grp for grp in duplicated_rot_groups], [grp for grp in rot_groups]]\n",
    "        with open(os.path.join(dest_dir, \"log.txt\"), \"w\") as f:\n",
    "            f.write(str(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
